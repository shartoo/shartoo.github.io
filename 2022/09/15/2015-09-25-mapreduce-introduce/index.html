<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="baidu-site-verification" content="93f8r6fzoB" />
<meta name="google-site-verification" content="TRFlJTt2XTd9bCvpogqNRWkuoxwFeOUBf8ouiChVFyQ" />
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 6.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/science_256px_1075043_easyicon.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/science_128px_1075043_easyicon.ico">
  <link rel="mask-icon" href="/images/stars.svg" color="#222">
  <meta name="google-site-verification" content="TRFlJTt2XTd9bCvpogqNRWkuoxwFeOUBf8ouiChVFyQ">
  <meta name="baidu-site-verification" content="93f8r6fzoB">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css">


<script id="hexo-configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    hostname: new URL('https://shartoo.github.io').hostname,
    root: '/',
    scheme: 'Pisces',
    version: '7.6.0',
    exturl: false,
    sidebar: {"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},
    copycode: {"enable":false,"show_result":false,"style":null},
    back2top: {"enable":true,"sidebar":false,"scrollpercent":false},
    bookmark: {"enable":false,"color":"#222","save":"auto"},
    fancybox: false,
    mediumzoom: false,
    lazyload: false,
    pangu: false,
    comments: {"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},
    algolia: {
      appID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    },
    localsearch: {"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},
    path: '',
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}
  };
</script>

  <meta name="description" content="spark计算框架">
<meta property="og:type" content="article">
<meta property="og:title" content="spark 测试">
<meta property="og:url" content="https://shartoo.github.io/2022/09/15/2015-09-25-mapreduce-introduce/index.html">
<meta property="og:site_name" content="数据与算法">
<meta property="og:description" content="spark计算框架">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://i.imgur.com/OWyTF1M.png">
<meta property="og:image" content="http://i.imgur.com/rtY99ub.png">
<meta property="og:image" content="http://i.imgur.com/nTBmin9.png">
<meta property="og:image" content="http://i.imgur.com/UE5Od8S.png">
<meta property="og:image" content="http://gitlab.hudoumiao.com/TopLevel/Knowledge_Base/uploads/ba527855ed3b360d8c82840c62b0b3ab/spark%E8%AE%A1%E7%AE%97code.png">
<meta property="og:image" content="http://gitlab.hudoumiao.com/TopLevel/Knowledge_Base/uploads/e82db4ea22a47be62bc7355505d06ba2/spark%E8%AE%A1%E7%AE%97code%E4%BE%9D%E8%B5%96.png">
<meta property="og:image" content="http://gitlab.hudoumiao.com/TopLevel/Knowledge_Base/uploads/8f0a48da38c9da65b84ed5af5262562f/spark%E6%8F%90%E4%BA%A4%E4%BD%9C%E4%B8%9A%E6%B5%81%E7%A8%8B.png">
<meta property="og:image" content="http://i.imgur.com/VdB2LPU.png">
<meta property="og:image" content="http://i.imgur.com/H5GA2XL.png">
<meta property="og:image" content="http://i.imgur.com/ya40qiL.png">
<meta property="og:image" content="http://i.imgur.com/7EatbDK.png">
<meta property="article:published_time" content="2022-09-15T08:16:12.746Z">
<meta property="article:modified_time" content="2022-09-15T08:16:12.746Z">
<meta property="article:author" content="shartoo">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://i.imgur.com/OWyTF1M.png">

<link rel="canonical" href="https://shartoo.github.io/2022/09/15/2015-09-25-mapreduce-introduce/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome: false,
    isPost: true
  };
</script>

  <title>spark 测试 | 数据与算法</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-meta">

    <div>
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">数据与算法</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
        <p class="site-subtitle">重新出发</p>
  </div>

  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>
</div>


<nav class="site-nav">
  
  <ul id="menu" class="menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-fw fa-home"></i>首页</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-fw fa-user"></i>关于</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-fw fa-tags"></i>标签</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-fw fa-th"></i>分类</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-fw fa-archive"></i>归档</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>

</nav>
  <div class="site-search">
    <div class="popup search-popup">
    <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocorrect="off" autocapitalize="none"
           placeholder="搜索..." spellcheck="false"
           type="text" id="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result"></div>

</div>
<div class="search-pop-overlay"></div>

  </div>
</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content">
            

  <div class="posts-expand">
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block " lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://shartoo.github.io/2022/09/15/2015-09-25-mapreduce-introduce/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/me.jpg">
      <meta itemprop="name" content="shartoo">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="数据与算法">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          spark 测试
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2022-09-15 16:16:12" itemprop="dateCreated datePublished" datetime="2022-09-15T16:16:12+08:00">2022-09-15</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/" itemprop="url" rel="index">
                    <span itemprop="name">大数据</span>
                  </a>
                </span>
            </span>

          
            <span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv" style="display: none;">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span id="busuanzi_value_page_pv"></span>
            </span>
            <div class="post-description">spark计算框架</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h2 id="spark-overview"><a href="#spark-overview" class="headerlink" title="spark overview"></a>spark overview</h2><h3 id="UC-Berkeley-的spark数据分析栈"><a href="#UC-Berkeley-的spark数据分析栈" class="headerlink" title="UC Berkeley 的spark数据分析栈"></a>UC Berkeley 的spark数据分析栈</h3><p><img src="http://i.imgur.com/OWyTF1M.png"></p>
<p>按使用方式划分</p>
<ul>
<li>离线批处理（Mlib，Graphs）</li>
<li>交互式查询（spark SQL）</li>
<li>时实计算（spark streaming）</li>
</ul>
<h3 id="spark资源调度"><a href="#spark资源调度" class="headerlink" title="spark资源调度"></a>spark资源调度</h3><p><img src="http://i.imgur.com/rtY99ub.png"></p>
<ul>
<li>stanalone</li>
<li>mesos</li>
<li>yarn</li>
</ul>
<p>  其中我们使用的是yarn资源调度，也就是运行spark job向集群申请资源的方式与hadoop是一样的，先向resourcemanger，然后在nodemanager，申请container启动applicationMaster,运行excutor</p>
<p>  yarn的提交job方式client和cluster</p>
<ul>
<li>client提交方式，driver program运行在提交机器上</li>
<li>cluster方式，driver program是运行在集群中的某个worker中</li>
</ul>
<h3 id="spark-VS-hadoop"><a href="#spark-VS-hadoop" class="headerlink" title="spark VS hadoop"></a>spark VS hadoop</h3><ul>
<li><p>应用场景</p>
<ul>
<li>hadoop的mapreduce适合做大数据集的离线批处理，</li>
<li>hadoop不是万能的，小数据集（单机能处理的小数据集杀鸡用牛刀），以及复杂的迭代运算，实时计算，在线分析等无能为力，而spark的出现很好的弥补了hadoop的不足之处，因为spark是基于内存的计算框架，适合复杂的迭代计算，spark streaming弥补实时计算的空缺（storm实时性更高，吞吐量，容错方面缺不如spark，稍后介绍spark的容错机制lineage和实时计算与storm的对比）</li>
</ul>
</li>
<li><p>运行效率</p>
<ul>
<li>spark官网效率比较<br><img src="http://i.imgur.com/nTBmin9.png"></li>
<li>咱门研究中心同事实际的测试报告</li>
</ul>
</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">   Spark性性能能与与MR相相比比较较提提高高了了13.6% </span><br><span class="line">    结果分析 </span><br><span class="line">    之前的Hadoop版本的批处理作业，共有23个作业，作业之间的关联方式为，</span><br><span class="line"> 前一个作业的输出结果保存在特定目录中，作为之后作业的输入数据。其中有一定量的计算结果是仅作为中间的临时数据存在，</span><br><span class="line">所有作业结束后将会被清理。这是由于每个Hadoop作业仅能执行一个MapReduce过程，</span><br><span class="line">这个问 题通过Spark的编程结构可以改善为按功能模块进行作业划分，每个作业中实现多个原来MapReduce的功能，</span><br><span class="line">将中间数据输出到磁盘并在下一次作业中重新读入的过程简化为Spark中的中间缓存变量保存在内存中。</span><br><span class="line">这里性能的提升主要来自于此，即优化了中间数据的冗余磁盘IO时间。此外，对于省网的分析作业而言，</span><br><span class="line">有着如下的特点，导致了性能提升不能达到理论上提及的一个数量级的改善效果。 </span><br><span class="line">    第一，原始数据量大，输入的基础数据量过于巨大，导致大量的时间花费在第一次的磁盘数据读取上，</span><br><span class="line">这个时间只取决于磁盘IO速率和文件大 小，而与分布式计算模式无关。省网分析作业内容大多属于磁盘密集型，</span><br><span class="line">与数据读取的时间相比，计算的时间耗费比重较轻，使得Hadoop和Spark的性能表现差异不大。 </span><br><span class="line">    第二，省网数据分析的内容，大多数属于单次的计算分析，即统计次数和汇总的工作，</span><br><span class="line">这方面Hadoop的性能可以极好的发挥出来。Spark更优势 于对一组小规模输入数据的，反复迭代计算，输入文件的读取时间较小，</span><br><span class="line">而计算过程十分复杂，这样其基于内存的计算方法可以更充分的展现优势。这在前一阶段中，</span><br><span class="line">使用分布式对矩阵进行计算的过程中体现的尤为明显，效果可以接近理论中提及的一个数量级提升。 </span><br><span class="line"></span><br></pre></td></tr></table></figure>

<ul>
<li>开发效率比较<ul>
<li>spark基于rdd的操作，是mapreduce的超集，提供我们基于rdd丰富的接口，如filter，disinct，reducebykey等等，而hadoop这些操作需要用户在map或reduce，combine自己编码实现，</li>
<li>咱门写mapreduce程序，每个job都要写maper类，reducer类（当然有些job可以不写reducer类，如sqoop导入数据库就只需maper），可能还要写partition，combiner类，而且写完job后，需要构建job与job之间执行的顺序和依赖关系，输入输出的键值类型等；</li>
<li>而spark是不需要这么琐碎，对rdd执行多个transform后，当执行一个action动作后（后面将介绍rdd的操作），自动构建一个基于rdd的DAG有向无环执行作业图，使用过pig的同事有所体会，这点类似pig，pig的解释器会将基于数据集的流处理过程，转换为DAG的job链，但spark又优于pig，可以做到过程控制，pig作为一个数据流语言，缺乏过程控制，粗糙的过程控制需要一门动态的脚本语言如python，javascript来实现，而且pig，hive只适合做统计分析作业，面对复杂的处理，如dougelas参数区线的压缩，需要用mapreduce或spark处理。</li>
</ul>
</li>
</ul>
<h3 id="开发语言支持"><a href="#开发语言支持" class="headerlink" title="开发语言支持"></a>开发语言支持</h3><ul>
<li>原生语言scala</li>
<li>java</li>
<li>python</li>
<li>spark1.4后支持R语言</li>
</ul>
<h3 id="spark的核心RDD"><a href="#spark的核心RDD" class="headerlink" title="spark的核心RDD"></a>spark的核心RDD</h3><p>  大家可以理解弹性分布式集合就是一个数据集合，这个集合有多个partition组成，而这些partition分布到集群中各节点的worker</p>
<ul>
<li><p>创建RDD的方式</p>
</li>
<li><p>基于内存集合<br>如1到100数字Range作为rdd，val data &#x3D; sc.parallelize(1 to 100)</p>
</li>
<li><p>外部存储系统，如hbase，cassandra，hdfs等， 如val data &#x3D; sc.textfile(“dataPath”)</p>
</li>
<li><p>基于rdd的操作</p>
<ul>
<li><p>Transformations操作<br>如map，filter，groupbykey等等，更多操作可参考<a target="_blank" rel="noopener" href="http://spark.apache.org/docs/latest/programming-guide.html#transformations">spark官网</a></p>
</li>
<li><p>action操作<br> top，count，reducebykey，saveastexrfile等等，更多操作可参考<a target="_blank" rel="noopener" href="http://spark.apache.org/docs/latest/programming-guide.html#actions">spark官网</a></p>
</li>
</ul>
</li>
</ul>
<p>  transform是lazy执行的，也就是说直到遇到该rdd链执行action操作，才会启动job，执行计算，这种思想跟scala语言的lazy十分相似，下面通过一个简单的scala例子体会下这种思想</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">package com.haohandata.dnsApp</span><br><span class="line">import scala.io.Source._</span><br><span class="line">import scala.io.Source</span><br><span class="line">/**</span><br><span class="line"> * @author xizhououyang@163.com</span><br><span class="line"> * @desription lazy deamon</span><br><span class="line"> */</span><br><span class="line">object LazyDeamon &#123;</span><br><span class="line">  /*</span><br><span class="line">代码解释：当我们输入一个不存在的文件，如果不执行for循环对文件进行读取，program并不会抛异常，也就是说定义一个变量为lazy</span><br><span class="line">后，当我们对其引用求值时候，才会加载运行，这点类似于java的反射机制，动态加载</span><br><span class="line">*/</span><br><span class="line">  def  main(args:Array[String])&#123;</span><br><span class="line">  lazy  val  file = Source.fromFile(&quot;/home/osun/pgadmin.logxx&quot;)</span><br><span class="line">  for(line&lt;- file.getLines)</span><br><span class="line">    println(line)</span><br><span class="line">  val word =&quot;learning spark&quot;</span><br><span class="line">   println(word)</span><br><span class="line">  </span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>


<h3 id="广播变量"><a href="#广播变量" class="headerlink" title="广播变量"></a>广播变量</h3><p>广播变量是分发到每个worker的只读变量不能修改，功能与hadoop的分布式缓存类似，</p>
<p>  目前的dns项目实战使用到是做资源表关联（大数据集与小数据集的关联），存放广播变量中，通过map转换操作做关联，注意广播变量是一个只读变量，不能做修改。</p>
<h3 id="计数器"><a href="#计数器" class="headerlink" title="计数器"></a>计数器</h3><p>作业中全局的一个计数器，与hadoop的计数器类似，并不陌生，我们平时跑完mr或者pig的时候会有三种类型计数器的统计，<br>Framkework计数器，job计数器，hdfs文件系统计数器，注意spark中的计数器是不能在task中求值，只能在driver program中求值</p>
<p>  在dns项目中统计各用户群，各运营商，top10的icp，每个icp下统计top10 的host，可先在每个partition中统计top10的icp和top10的host，然后保存到计数器变量中，然后将聚合后结果话单过滤只保留掉计数器中的host和icp，这样可以避免多次迭代调用rdd.top（10）产生N<em>N个job；取五分钟小片数据，采用n</em>n迭代调用rdd.top方式生成库表需要两个小时，并产生了1800多个小job，跑了两个多小时，采用计数器过滤方式，4分多钟就能跑完库表实现入库postgresql</p>
<h3 id="rdd依赖"><a href="#rdd依赖" class="headerlink" title="rdd依赖"></a>rdd依赖</h3><ul>
<li>narrow依赖（父rdd的同一个partion最多只给子rdd一个partion依赖）</li>
<li>wide依赖（父rdd的同一个partion被子rdd多个partion依赖）<br><img src="http://i.imgur.com/UE5Od8S.png"></li>
</ul>
<h3 id="小结，从计算，存储，容错谈谈rdd"><a href="#小结，从计算，存储，容错谈谈rdd" class="headerlink" title="小结，从计算，存储，容错谈谈rdd"></a>小结，从计算，存储，容错谈谈rdd</h3><ul>
<li>计算</li>
</ul>
<p><img src="http://gitlab.hudoumiao.com/TopLevel/Knowledge_Base/uploads/ba527855ed3b360d8c82840c62b0b3ab/spark%E8%AE%A1%E7%AE%97code.png" alt="spark计算code"></p>
<p>注意：由于时间关系，直接截了他人画的图，deamon中存在一点error，正确的代码应该是map(parts&#x3D;&gt;(parts(0),parts(1).toInt)),第一次map的transform得到的是RDD[Array[String]],不是RDD[List[String]]</p>
<p>code.png)<img src="http://gitlab.hudoumiao.com/TopLevel/Knowledge_Base/uploads/e82db4ea22a47be62bc7355505d06ba2/spark%E8%AE%A1%E7%AE%97code%E4%BE%9D%E8%B5%96.png" alt="spark计算code依赖"></p>
<p>每个job划分不同的stage，每个stage就是一个Set[task]集合  </p>
<p><img src="http://gitlab.hudoumiao.com/TopLevel/Knowledge_Base/uploads/8f0a48da38c9da65b84ed5af5262562f/spark%E6%8F%90%E4%BA%A4%E4%BD%9C%E4%B8%9A%E6%B5%81%E7%A8%8B.png" alt="spark提交作业流程"></p>
<p>  spark的作业调度，分DAGshedule，和taskshedule二级，跟hadoop的jobtraker，tasktracker两级调度类似</p>
<ul>
<li><p>存储</p>
</li>
<li><p>MEMORY_ONLY</p>
</li>
<li><p>MEMORY_AND_DISK</p>
</li>
<li><p>MEMORY_ONLY_SER</p>
</li>
<li><p>MEMORY_AND_DISK_SER</p>
</li>
<li><p>DISK_ONLY</p>
</li>
<li><p>MEMORY_ONLY_2, MEMORY_AND_DISK_2</p>
</li>
</ul>
<p>  上面是spark是rdd的各种存储策略，是spark计算框架中，默认认为重复计算rdd需要的时间会比从磁盘中读取数据进行的io操作效率高，<br> 因此默认所有的rdd的persist方式都是存在内存中，当内存不足后，会丢弃掉这个rdd，需要时候再根据lineage<br>机制从新计算，实际开发中那如果认为计算出来的rdd代价远比进行io大，这时可根据情况选择其他持久化策略，如在dns项目中，需要关联ppp的result和record话单后的rdd，采取MEMORY_AND_DISK_SER方式的持久化</p>
<ul>
<li>容错（lineage）</li>
</ul>
<p>穿插一个小故事：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">  高帅富小明家有一个家传宝，祖训这个宝物得一代代往下传，每代人可能对这个传家宝，</span><br><span class="line">每代人需对这宝物进行雕塑改造，如嵌入宝石，或者砖石，某天小明炒股亏空了，于是他要变卖这个传家宝，</span><br><span class="line">可是造化弄人，当他要变卖时候，发现传家宝不见了，聪明的小明，首先会确认他爸爸是否已经把这件宝物传了给他，</span><br><span class="line">如果确定是，他会将在翻遍自己房子找，如果他父亲没传给他，直接去他父亲的住处找，按照这个步骤，</span><br><span class="line">如果祖父那还没找到，他会一直回溯到他曾祖父那，直到找到传家宝，然后再一代代地传给小明，</span><br><span class="line">小明得到宝物后最终把它变卖</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>分析情景:</p>
<ul>
<li>rdd就好比传家宝</li>
<li>情景中的每个人物就好比不同时候集群中的计算节点中的worker</li>
<li>小明变卖宝物，就好比执行了一个action，触发提交job</li>
<li>而每代人对宝物加入一个宝石，就好比rdd的transform操作</li>
<li>rdd的容错是lineage机制，如果当向spark提交job的时候，会构造基于rdd操作的DAG的作业流，这时会有基于rdd依赖链，如果计算过程中某个rdd丢失了，它会从父rdd那重新计算，如果父rdd不存在，会一直回溯上去直到找到父的rdd，然后再依照依赖链重新执行计算，最后执行action操作</li>
</ul>
<h2 id="spark在项目的实战应用"><a href="#spark在项目的实战应用" class="headerlink" title="spark在项目的实战应用"></a>spark在项目的实战应用</h2><h3 id="架构图"><a href="#架构图" class="headerlink" title="架构图"></a>架构图</h3><p><img src="http://i.imgur.com/VdB2LPU.png"></p>
<h3 id="项目代码"><a href="#项目代码" class="headerlink" title="项目代码"></a>项目代码</h3><p><a target="_blank" rel="noopener" href="http://gitlab.hudoumiao.com/applications/User_Mobility_Analysis/tree/master/sparkcode/src/main/scala/com/haohandata">http://gitlab.hudoumiao.com/applications/User_Mobility_Analysis/tree/master/sparkcode/src/main/scala/com/haohandata</a></p>
<h2 id="spark-streaming"><a href="#spark-streaming" class="headerlink" title="spark streaming"></a>spark streaming</h2><h3 id="spark-streaming-vs-Storm（下面是引用研究中心同事的给出的两者对比的报告内容）"><a href="#spark-streaming-vs-Storm（下面是引用研究中心同事的给出的两者对比的报告内容）" class="headerlink" title="spark streaming vs Storm（下面是引用研究中心同事的给出的两者对比的报告内容）"></a>spark streaming vs Storm（下面是引用研究中心同事的给出的两者对比的报告内容）</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"> Storm和Spark Streaming都是分布式流处理的开源框架。虽然二者功能类似，但是也有着一定的区别。 </span><br><span class="line"></span><br><span class="line">- 处理模型 </span><br><span class="line"></span><br><span class="line">虽然这两个框架都提供可扩展性和容错性,它们根本的区别在于他们的处理模型。 </span><br><span class="line">Spark Streaming是将流式计算分解成一系列短小的批处理作业。这里的批处理引擎是Spark，</span><br><span class="line">也就是把Spark Streaming的输入数据按照batch size  （如1秒）分成一段一段的数据 （Discretized Stream），</span><br><span class="line">每一段数据都 转换成Spark中的RDD  （Resilient Distributed Dataset ），然后将Spark Streaming 中对DStream的Transformation操作变为针对Spark中对RDD的Transformation操作，将RDD经过操作变成中间结果保存在内存中。整个流式计算根据业务的需求可以对中间的结果进行叠加，或者存储到外部设备。 </span><br><span class="line"> </span><br><span class="line">在Storm中，先要设计一个用于实时计算的图状结构，我们称之为拓扑 （topology ）。</span><br><span class="line">这个拓扑将会被提交给集群，由集群中的主控节点 （masternode ）分发代码，将任务分配给工作节点 （worker node ）执行。</span><br><span class="line">一个拓扑中包括spout和bolt两种角色，其中spout发送消息，负责将数据流以tuple元 组的形式发送出去；</span><br><span class="line">而bolt则负责转发数据流，在bolt 中可以完成计算、过滤等操作，bolt 自身也可以随机将数据发送给其他bolt 。</span><br><span class="line">在storm中，每个都是tuple是不可变数组，对应着固定的键值对。 简而言之，Storm是让数据面向计算，</span><br><span class="line">而Spark Streaming是使计算面向数据。 </span><br><span class="line"></span><br><span class="line">- 延迟，storm更高</span><br><span class="line">Spark Streaming，最小的Batch Size的选取在0.5~2秒钟之间，而Storm 目前最小的延迟是100ms左右，</span><br><span class="line">所以Spark Streaming能，够满足除对实时性要求非常高 （如高频实时交易）之外的所有流式准实时计算场景，</span><br><span class="line">而高实时性要求的场景则应该交给Storm来完成。 </span><br><span class="line"></span><br><span class="line">- 容错，spark streaming更好 </span><br><span class="line"></span><br><span class="line">在容错数据保证方面的权衡是，Spark Streaming提供了更好的支持容错状态计算。</span><br><span class="line">在Storm中,每个单独的记录当它通过系统时必须被跟踪，所以 Storm能够至少保证每个记录将被处理一次，</span><br><span class="line">但是在从错误中恢复过来时候允许出现重复记录。这意味着可变状态可能不正确地被更新两次。 </span><br><span class="line">另一方 面，Spark Streaming只需要在批级别进行跟踪处理，因此可以有效地保证每个mini-batch将完全被处理一次，</span><br><span class="line">即便一个节点发生故障。 </span><br><span class="line"></span><br><span class="line">- 吞吐量，spark streaming更强 </span><br><span class="line">   Spark 目前在EC2上已能够线性扩展到100个节点 （每个节点4Core ），可以以数秒的延迟处理6GB/s的数据量 （60M records/s ），其吞吐量也比流行的Storm高2～5倍。 </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">使用选择 </span><br><span class="line"></span><br><span class="line">如果你想要的是一个允许增量计算的高速事件处理系统，Storm会是最佳选择。</span><br><span class="line">它可以应对你在客户端等待结果的同时，进一步进行分布式计算的需求，使用开箱即用的分布式RPC  （DRPC）就可以了。</span><br><span class="line">最后但同样重要的原因：Storm使用Apache Thrift ，你可以用任何编程语言来编写拓扑结构。</span><br><span class="line">如果你需要状态持续，同时/或者达到恰好一次的传递效果，应当看看更高层面的Trdent API，它同时也提供了微批处理的方式。 </span><br><span class="line">如果你必须有状态的计算，恰好一次的递送，并且不介意高延迟的话，那么可以考虑Spark Streaming，</span><br><span class="line">特别如果你还计划图形操作、机器学习或者访问SQL的话，ApacheSpark的stack允许你将一些library与数据流相结合 </span><br><span class="line">（Spark SQL，Mllib，GraphX），它们会提供便捷的一体化编程模型。</span><br><span class="line">尤其是数据流算法 （例如：K均值流媒体）允许Spark实时决策的促进。 </span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h3 id="核心DStream"><a href="#核心DStream" class="headerlink" title="核心DStream"></a>核心DStream</h3><ul>
<li>Dstream简介<br>Dstream是一组以时间为轴连续的一组rdd<br><img src="http://i.imgur.com/H5GA2XL.png"></li>
<li>Dstream的输入源</li>
</ul>
<p><img src="http://i.imgur.com/ya40qiL.png"></p>
<ul>
<li>DStream的transformations操作</li>
<li>DSstream的action操作</li>
</ul>
<h3 id="使用场景划分"><a href="#使用场景划分" class="headerlink" title="使用场景划分"></a>使用场景划分</h3><ul>
<li>无状态</li>
</ul>
<p>每次批处理，receiver接收的数据都作为数据Dstream操作</p>
<ul>
<li>有状态updateStateByKey(func)</li>
</ul>
<p>  本次计算，需要用到上次批处理的结果。<br>比如spark streaming的批处理时间是五分钟，但业务中，我需要统计话单中haohandata.com.cn从程序运行后，每五分钟后haohandata.com.cn这个域名的累加的访问数，这时我们会以上次批处理为key的访问次数，加上本次五分钟批处理得到结果</p>
<ul>
<li>windowns</li>
</ul>
<p>基于窗口的操作，批处理时间，滑动窗口，窗口大小<br>DNS实时计算实验项目中，统计五分钟粒度各rcode的次分布，<br>由于存在边界数据，解决的办法采取五分钟为批处理时间，滑动窗口为五分钟，窗口大小为10分钟，每次进行reduceByKeyAndWindow后，会进行过滤，只存这个windown中的中间五分钟数据，再入库cassandra</p>
<h2 id="dns项目的spark-streaming实时计算（实验性项目）"><a href="#dns项目的spark-streaming实时计算（实验性项目）" class="headerlink" title="dns项目的spark streaming实时计算（实验性项目）"></a>dns项目的spark streaming实时计算（实验性项目）</h2><h3 id="DNS项目处理流程图"><a href="#DNS项目处理流程图" class="headerlink" title="DNS项目处理流程图"></a>DNS项目处理流程图</h3><p><img src="http://i.imgur.com/7EatbDK.png"></p>
<h3 id="项目代码-1"><a href="#项目代码-1" class="headerlink" title="项目代码"></a>项目代码</h3>
    </div>

    
    
    

      <footer class="post-footer">

        


        
    <div class="post-nav">
      <div class="post-nav-item"></div>
      <div class="post-nav-item">
    <a href="/2022/09/15/2016-03-10-kafkaquestion/" rel="next" title="大数据：kafka常见问题">
      大数据：kafka常见问题 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  

  </div>


          </div>
          
    <div class="comments" id="gitalk-container"></div>

<script>
  window.addEventListener('tabs:register', () => {
    let activeClass = CONFIG.comments.activeClass;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#spark-overview"><span class="nav-number">1.</span> <span class="nav-text">spark overview</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#UC-Berkeley-%E7%9A%84spark%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E6%A0%88"><span class="nav-number">1.1.</span> <span class="nav-text">UC Berkeley 的spark数据分析栈</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#spark%E8%B5%84%E6%BA%90%E8%B0%83%E5%BA%A6"><span class="nav-number">1.2.</span> <span class="nav-text">spark资源调度</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#spark-VS-hadoop"><span class="nav-number">1.3.</span> <span class="nav-text">spark VS hadoop</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%BC%80%E5%8F%91%E8%AF%AD%E8%A8%80%E6%94%AF%E6%8C%81"><span class="nav-number">1.4.</span> <span class="nav-text">开发语言支持</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#spark%E7%9A%84%E6%A0%B8%E5%BF%83RDD"><span class="nav-number">1.5.</span> <span class="nav-text">spark的核心RDD</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%B9%BF%E6%92%AD%E5%8F%98%E9%87%8F"><span class="nav-number">1.6.</span> <span class="nav-text">广播变量</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%AE%A1%E6%95%B0%E5%99%A8"><span class="nav-number">1.7.</span> <span class="nav-text">计数器</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#rdd%E4%BE%9D%E8%B5%96"><span class="nav-number">1.8.</span> <span class="nav-text">rdd依赖</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%B0%8F%E7%BB%93%EF%BC%8C%E4%BB%8E%E8%AE%A1%E7%AE%97%EF%BC%8C%E5%AD%98%E5%82%A8%EF%BC%8C%E5%AE%B9%E9%94%99%E8%B0%88%E8%B0%88rdd"><span class="nav-number">1.9.</span> <span class="nav-text">小结，从计算，存储，容错谈谈rdd</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#spark%E5%9C%A8%E9%A1%B9%E7%9B%AE%E7%9A%84%E5%AE%9E%E6%88%98%E5%BA%94%E7%94%A8"><span class="nav-number">2.</span> <span class="nav-text">spark在项目的实战应用</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%9E%B6%E6%9E%84%E5%9B%BE"><span class="nav-number">2.1.</span> <span class="nav-text">架构图</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%A1%B9%E7%9B%AE%E4%BB%A3%E7%A0%81"><span class="nav-number">2.2.</span> <span class="nav-text">项目代码</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#spark-streaming"><span class="nav-number">3.</span> <span class="nav-text">spark streaming</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#spark-streaming-vs-Storm%EF%BC%88%E4%B8%8B%E9%9D%A2%E6%98%AF%E5%BC%95%E7%94%A8%E7%A0%94%E7%A9%B6%E4%B8%AD%E5%BF%83%E5%90%8C%E4%BA%8B%E7%9A%84%E7%BB%99%E5%87%BA%E7%9A%84%E4%B8%A4%E8%80%85%E5%AF%B9%E6%AF%94%E7%9A%84%E6%8A%A5%E5%91%8A%E5%86%85%E5%AE%B9%EF%BC%89"><span class="nav-number">3.1.</span> <span class="nav-text">spark streaming vs Storm（下面是引用研究中心同事的给出的两者对比的报告内容）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%A0%B8%E5%BF%83DStream"><span class="nav-number">3.2.</span> <span class="nav-text">核心DStream</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BD%BF%E7%94%A8%E5%9C%BA%E6%99%AF%E5%88%92%E5%88%86"><span class="nav-number">3.3.</span> <span class="nav-text">使用场景划分</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#dns%E9%A1%B9%E7%9B%AE%E7%9A%84spark-streaming%E5%AE%9E%E6%97%B6%E8%AE%A1%E7%AE%97%EF%BC%88%E5%AE%9E%E9%AA%8C%E6%80%A7%E9%A1%B9%E7%9B%AE%EF%BC%89"><span class="nav-number">4.</span> <span class="nav-text">dns项目的spark streaming实时计算（实验性项目）</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#DNS%E9%A1%B9%E7%9B%AE%E5%A4%84%E7%90%86%E6%B5%81%E7%A8%8B%E5%9B%BE"><span class="nav-number">4.1.</span> <span class="nav-text">DNS项目处理流程图</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%A1%B9%E7%9B%AE%E4%BB%A3%E7%A0%81-1"><span class="nav-number">4.2.</span> <span class="nav-text">项目代码</span></a></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="shartoo"
      src="/images/me.jpg">
  <p class="site-author-name" itemprop="name">shartoo</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">102</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">10</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">4</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">shartoo</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> 强力驱动 v6.3.0
  </div>
  <span class="post-meta-divider">|</span>
  <div class="theme-info">主题 – <a href="https://pisces.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Pisces</a> v7.6.0
  </div>

        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="post-meta-item" id="busuanzi_container_site_uv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item" id="busuanzi_container_site_pv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>


  <script defer src="/lib/three/three.min.js"></script>
    <script defer src="/lib/three/three-waves.min.js"></script>


  
  <script>
    (function(){
      var bp = document.createElement('script');
      var curProtocol = window.location.protocol.split(':')[0];
      bp.src = (curProtocol === 'https') ? 'https://zz.bdstatic.com/linksubmit/push.js' : 'http://push.zhanzhang.baidu.com/push.js';
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(bp, s);
    })();
  </script>




  
<script src="/js/local-search.js"></script>













  

  
      
<script type="text/x-mathjax-config">

  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$', '$'], ['\\(', '\\)'] ],
      processEscapes: true,
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    },
    TeX: {
      equationNumbers: {
        autoNumber: 'AMS'
      }
    }
  });

  MathJax.Hub.Register.StartupHook('TeX Jax Ready', function() {
    MathJax.InputJax.TeX.prefilterHooks.Add(function(data) {
      if (data.display) {
        var next = data.script.nextSibling;
        while (next && next.nodeName.toLowerCase() === '#text') {
          next = next.nextSibling;
        }
        if (next && next.nodeName.toLowerCase() === 'br') {
          next.parentNode.removeChild(next);
        }
      }
    });
  });

  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for (i = 0; i < all.length; i += 1) {
      element = document.getElementById(all[i].inputID + '-Frame').parentNode;
      if (element.nodeName.toLowerCase() == 'li') {
        element = element.parentNode;
      }
      element.classList.add('has-jax');
    }
  });
</script>
<script>
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/mathjax@2/MathJax.js?config=TeX-AMS-MML_HTMLorMML', () => {
    MathJax.Hub.Typeset();
  }, window.MathJax);
</script>

    

  

<link rel="stylesheet" href="//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.css">

<script>
NexT.utils.loadComments(document.querySelector('#gitalk-container'), () => {
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js', () => {
    var gitalk = new Gitalk({
      clientID: 'c656cd038e01f710e260',
      clientSecret: 'e6de2ccaaf0f7069292125b8f50e27f25b95810d',
      repo: 'shartoo.github.io',
      owner: 'shartoo',
      admin: ['shartoo'],
      id: '8a29218af49b682e7c67af3c5c904068',
        language: '',
      distractionFreeMode: 'true'
    });
    gitalk.render('gitalk-container');
  }, window.Gitalk);
});
</script>

  <!-- 页面点击小红心 -->
<script type="text/javascript" src="/js/src/clicklove.js"></script>
</body>
</html>
